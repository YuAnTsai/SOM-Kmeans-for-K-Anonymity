{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from minisom import MiniSom\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    # Load Adult dataset and seperate to features(X) and target(y)\n",
    "    def __init__(self, path='D:/昱安在政大/108學年度上學期課程/資安/data/adult.csv'):\n",
    "        df = shuffle(pd.read_csv(path, engine='python'))\n",
    "        df = self.clean(df)\n",
    "\n",
    "        self.y = df.pop('income')\n",
    "        self.X = df\n",
    "\n",
    "        # Label encode y\n",
    "        self.y_encoder = LabelEncoder()\n",
    "        self.y = self.y_encoder.fit_transform(self.y)\n",
    "\n",
    "        # One Hot encode X\n",
    "        self.X = pd.get_dummies(self.X)\n",
    "\n",
    "        for name in self.X.columns:\n",
    "            if self.X[name].dtype == 'object':\n",
    "                self.X[name] = self.X[name].astype('category')\n",
    "\n",
    "    def clean(self, df):\n",
    "        return df.replace('?', np.nan).dropna().drop('fnlwgt', axis=1)\n",
    "\n",
    "    def train_test_split(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2)\n",
    "        y_train = pd.Series(y_train, index=X_train.index)\n",
    "        y_test = pd.Series(y_test, index=X_test.index)\n",
    "        return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data + SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "som_start=datetime.now()\n",
    "\n",
    "data = Data()\n",
    "\n",
    "quasi_identifiers = ['age', 'educational-num',\n",
    "                         'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "features = data.X[quasi_identifiers]\n",
    "target = data.y\n",
    "\n",
    "#要調整\n",
    "width=150\n",
    "height=150\n",
    "sigma=.9\n",
    "lr=.2\n",
    "epochs=1e5\n",
    "#epochs = 10\n",
    "\n",
    "verbose=True\n",
    "log = 1000\n",
    "\n",
    "som = MiniSom(width, height, features.shape[1], sigma, lr)\n",
    "som.train_random(features.values, int(epochs), verbose=True)\n",
    "\n",
    "out = []\n",
    "for step, (X, y) in enumerate(zip(features.values, target)):\n",
    "    new_X = som.winner(X)\n",
    "    out.append((new_X, X, y))\n",
    "    if(verbose == True and step % log == 0):\n",
    "        print(f'*Creating SOM: [{step}/{features.shape[0]}]')\n",
    "som_data = np.array(out)\n",
    "\n",
    "new_data = []\n",
    "new_X = []\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(som_data[:,0])):\n",
    "    new_X.append(np.asarray(som_data[:,0][i]))\n",
    "    X.append(np.asarray(som_data[:,1][i]))\n",
    "    y.append(np.asarray(som_data[:,2][i]))\n",
    "new_data = (new_X, X, y)\n",
    "\n",
    "print(\"Time required for SOM: \" + str(datetime.now()-som_start))\n",
    "\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingModel:\n",
    "    def __init__(self, input_shape):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(64, activation='relu', input_shape=input_shape))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(128, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(128, activation='relu'))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, data, label):\n",
    "        self.model.fit(data, label, epochs=1, batch_size=128, verbose=0)\n",
    "\n",
    "    def predict(self, data):\n",
    "        return self.model.predict_classes(data)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, print_report=True):\n",
    "        y_predicted = self.predict(X_test)\n",
    "        y_predicted_probs = self.model.predict_proba(X_test)\n",
    "        if print_report:\n",
    "            self.print_report(y_test, y_predicted, y_predicted_probs)\n",
    "        else:\n",
    "            accuracy = accuracy_score(y_test, y_predicted)\n",
    "            report = classification_report(y_test, y_predicted, output_dict=True)\n",
    "            auc_score = roc_auc_score(y_test, y_predicted_probs)\n",
    "            matrix = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "            return {\n",
    "                'accuracy': accuracy,\n",
    "                'auc_score': auc_score,\n",
    "                **report['weighted avg'],\n",
    "            }\n",
    "\n",
    "    def print_report(self, test, predicted, predicted_probs):\n",
    "        accuracy = accuracy_score(test, predicted)\n",
    "        report = classification_report(test, predicted)\n",
    "        matrix = confusion_matrix(test, predicted)\n",
    "\n",
    "        print('Accuracy score: {:.5f}'.format(accuracy))\n",
    "        print('-' * 20)\n",
    "        print('Confusion Matrix:')\n",
    "        print(matrix)\n",
    "        print('-' * 20)\n",
    "        print(report)\n",
    "        print('-' * 20)\n",
    "        print('AUC score: {:.5f}'.format(roc_auc_score(test, predicted_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#將SOM資料集丟入Kmeans\n",
    "#Kmeans分群數依照k值(k-anonymity的k)決定，從K = x(資料總比數)/k(理想上)開始遞減直到每一群至少有k筆資料\n",
    "#計算跑的時間\n",
    "\n",
    "sizes = [5, 10, 15, 20, 30, 50]\n",
    "#k = 5\n",
    "#K = int(len(new_data[0])//k)\n",
    "#D = 500//k #Decline\n",
    "#cluster = True\n",
    "#c = Counter()\n",
    "\n",
    "for k in sizes:\n",
    "    \n",
    "    perturb_start=datetime.now()\n",
    "    \n",
    "    print(\"K-Anonymity: k=\" + str(k))\n",
    "    K = int(len(new_data[0])//k) #Set the initial number of clusters\n",
    "    D = 500//k #Set how many clusters to drop for every next loop\n",
    "    cluster = True\n",
    "    while cluster:\n",
    "        clf = KMeans(n_clusters=K)\n",
    "        clf.fit(new_data[0])\n",
    "        c = Counter(clf.labels_)\n",
    "        print(\"K=\" + str(K))\n",
    "        for i in range(0,K):\n",
    "            print(\"cluster \" + str(i) + \" has \" + str(c[i]) + \" data points\")\n",
    "            if c[i]<k:\n",
    "                break\n",
    "            else:\n",
    "                if i == K-1:\n",
    "                    cluster = False\n",
    "                else:\n",
    "                    pass\n",
    "        K = K-D\n",
    "    K = K+D\n",
    "    print(\"The Resulting number of clusters: \" + str(K))\n",
    "    \n",
    "    #將分完群的資料還原回原始資料的feature\n",
    "    #根據分群結果perturb\n",
    "    #準備丟入神經網路\n",
    "\n",
    "    data = pd.concat([pd.DataFrame(new_data[1],columns=['age', 'educational-num','capital-gain', 'capital-loss', 'hours-per-week']), \n",
    "                      pd.DataFrame(new_data[2],columns=['target']), pd.DataFrame(clf.labels_,columns=['cluster'])], axis = 1)\n",
    "    columns = ['age', 'educational-num','capital-gain', 'capital-loss', 'hours-per-week','target','cluster']\n",
    "    index = range(0,len(data))\n",
    "    data_perturbed = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        for j in range(0,K+1):\n",
    "            if data['cluster'][i] == j:\n",
    "                data_perturbed[columns[0]][i] = float(data.groupby(by='cluster').mean()[columns[0]][j])\n",
    "                data_perturbed[columns[1]][i] = float(data.groupby(by='cluster').mean()[columns[1]][j])\n",
    "                data_perturbed[columns[2]][i] = float(data.groupby(by='cluster').mean()[columns[2]][j])\n",
    "                data_perturbed[columns[3]][i] = float(data.groupby(by='cluster').mean()[columns[3]][j])\n",
    "                data_perturbed[columns[4]][i] = float(data.groupby(by='cluster').mean()[columns[4]][j])\n",
    "                data_perturbed['target'][i] = data['target'][i]\n",
    "                data_perturbed['cluster'][i] = data['cluster'][i]\n",
    "\n",
    "    data_sorted = data_perturbed.sort_values(by=['cluster'])\n",
    "    data_ready = data_sorted.drop('cluster', axis=1)\n",
    "    \n",
    "    print(\"Time required for perturbation: \" + str(datetime.now()-perturb_start))\n",
    "    \n",
    "    #神經網路\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_ready.iloc[:,0:5], np.array(data_ready50.iloc[:,5:], dtype=int), test_size=0.2)\n",
    "    model = TrainingModel((5,))\n",
    "    model.fit(X_train, y_train)\n",
    "    model.evaluate(X_test, y_test, print_report=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
